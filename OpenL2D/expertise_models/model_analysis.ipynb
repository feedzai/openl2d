{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "#hi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import metrics\n",
    "from aequitas.group import Group\n",
    "\n",
    "from autodefer.models import haic\n",
    "from autodefer.utils import thresholding as t, plotting\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "cfg_path = 'cfg.yaml'\n",
    "with open(cfg_path, 'r') as infile:\n",
    "    cfg = yaml.safe_load(infile)\n",
    "\n",
    "\n",
    "data_cfg_path = '../data/dataset_cfg.yaml'\n",
    "\n",
    "with open(data_cfg_path, 'r') as infile:\n",
    "    data_cfg = yaml.safe_load(infile)\n",
    "\n",
    "\n",
    "cat_dict = data_cfg['categorical_dict']\n",
    "\n",
    "LABEL_COL = 'fraud_label'\n",
    "PROTECTED_COL = 'customer_age'\n",
    "CATEGORICAL_COLS = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os', 'assignment']\n",
    "TIMESTAMP_COL = 'month'\n",
    "SCORE_COL = 'model_score'\n",
    "BATCH_COL = 'batch'\n",
    "ASSIGNMENT_COL = 'assignment'\n",
    "DECISION_COL = 'decision'\n",
    "\n",
    "\n",
    "expert_ids_path = '/mnt/home/jean.alves/Clean_May/experts/expert_info_30_05_2023/expert_ids.yaml'\n",
    "\n",
    "with open(expert_ids_path, 'r') as infile:\n",
    "    EXPERT_IDS = yaml.safe_load(infile)\n",
    "#EXPERT_IDS = metadata['expert_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert behavior model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMAs = dict()\n",
    "\n",
    "for train_env in os.listdir(MODELS_PATH):\n",
    "    RMAs[train_env] = haic.assigners.RiskMinimizingAssigner(\n",
    "        expert_ids=EXPERT_IDS,\n",
    "        outputs_dir=f'{MODELS_PATH}{train_env}/human_expertise_model/',\n",
    "    )\n",
    "    calibrator_path = f'{MODELS_PATH}{train_env}/calibrator.pickle'\n",
    "    RMAs[train_env].load(CATEGORICAL_COLS, SCORE_COL, ASSIGNMENT_COL, calibrator_path, cat_dict)\n",
    "\n",
    "#Data and Models are now Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use this function to obtain the outcomes of each expert's prediction. \n",
    "test = pd.read_parquet('../testbed/test/test.parquet')\n",
    "test_expert_pred = pd.read_parquet('../testbed/test/test_expert_pred.parquet')\n",
    "\n",
    "def cat_checker(data, features, cat_dict):\n",
    "    new_data = data.copy()\n",
    "    for feature in features:\n",
    "        if new_data[feature].dtype != 'category':\n",
    "            new_data[feature] = pd.Categorical(new_data[feature].values, categories=cat_dict[feature])\n",
    "        elif new_data[feature].dtype.categories.to_list() != cat_dict[feature]:\n",
    "            new_data[feature] = pd.Categorical(new_data[feature].values, categories=cat_dict[feature])\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "cat_dict['assignment'] = EXPERT_IDS['human_ids'] + EXPERT_IDS['model_ids']\n",
    "\n",
    "test = cat_checker(test, data_cfg['data_cols']['categorical'], cat_dict)\n",
    "\n",
    "def get_outcome(label, pred):\n",
    "    if pred == 1:\n",
    "        if label == 1:\n",
    "            o = 'tp'\n",
    "        elif label == 0:\n",
    "            o = 'fp'\n",
    "    elif pred == 0:\n",
    "        if label == 1:\n",
    "            o = 'fn'\n",
    "        elif label == 0:\n",
    "            o = 'tn'\n",
    "    return o\n",
    "\n",
    "#The models take as input a dataframe consisting of features, followed by model_score, followed by assignment\n",
    "test['assignment'] = 'blank'\n",
    "\n",
    "outcomes = pd.DataFrame()\n",
    "outcomes['labels'] = test['fraud_label']\n",
    "test = test.drop(columns = ['fraud_label', 'month'])\n",
    "roc_curves = dict()\n",
    "if os.path.isfile('roc_curves.pkl'):\n",
    "    with open('roc_curves.pkl', 'rb') as fp:\n",
    "        roc_curves = pickle.load(fp)\n",
    "else:\n",
    "    for env_id in os.listdir(MODELS_PATH):\n",
    "        print(env_id)\n",
    "        if env_id != 'small_regular':\n",
    "            continue\n",
    "        curves = dict()\n",
    "        for expert in EXPERT_IDS['human_ids']:\n",
    "            model = RMAs[env_id]\n",
    "            outcomes['decisions'] = test_expert_pred[expert]\n",
    "            outcomes['outcomes'] = outcomes.apply(lambda x: get_outcome(label=x['labels'], pred=x['decisions']),\n",
    "                axis=1,\n",
    "            )\n",
    "            \n",
    "            test['assignment'] = expert\n",
    "            \n",
    "            test = cat_checker(test, data_cfg['data_cols']['categorical'] + ['assignment'], cat_dict)\n",
    "\n",
    "            pred_proba = model.expert_model.predict_proba(test)\n",
    "            fp_fpr, fp_tpr,_ = metrics.roc_curve(y_true = (outcomes['outcomes'] == 'fp').astype(int), y_score = pred_proba[:, model.expert_model.classes_ == 'fp'].squeeze())\n",
    "            fn_fpr, fn_tpr,_ = metrics.roc_curve(y_true = (outcomes['outcomes'] == 'fn').astype(int), y_score = pred_proba[:, model.expert_model.classes_ == 'fn'].squeeze())\n",
    "            \n",
    "            curves[expert]= {'fp_fpr': fp_fpr,\n",
    "                            'fp_tpr': fp_tpr,\n",
    "                            'fn_fpr': fn_fpr,\n",
    "                            'fn_tpr': fn_tpr,\n",
    "                            }\n",
    "            \n",
    "        roc_curves[env_id] = curves\n",
    "    with open('roc_curves.pkl', 'wb') as fp:\n",
    "        pickle.dump(roc_curves, fp)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette(['C0','C2', 'C3'])\n",
    "pal = pal.as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "print(roc_curves.keys())\n",
    "train_env = 'small_regular'\n",
    "exp = roc_curves[train_env]\n",
    "\n",
    "\n",
    "reg_counter = 0\n",
    "ma_counter = 0\n",
    "un_counter = 0\n",
    "for expert in EXPERT_IDS['human_ids']:\n",
    "    setlabel = None\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        color = pal[0]\n",
    "        if(reg_counter == 0):\n",
    "            setlabel = 'standard'\n",
    "            reg_counter +=1\n",
    "    elif expert.split('#')[0] == 'model_agreeing':\n",
    "        color = pal[1]\n",
    "        if(ma_counter == 0):\n",
    "            setlabel = 'agreeing'\n",
    "            ma_counter+=1\n",
    "    elif expert.split('#')[0] == 'unfair':\n",
    "        color = pal[2]\n",
    "        if(un_counter == 0):\n",
    "            setlabel = 'unfair'\n",
    "            un_counter += 1\n",
    "    plt.plot(exp[expert]['fp_fpr'],exp[expert]['fp_tpr'], c = color, linewidth = 0.4, label = setlabel)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0,1,0.01), np.arange(0,1,0.01), c = 'gray', linestyle = 'dashed')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title(f'ROC - False Positives - One vs. Others')\n",
    "plt.legend()\n",
    "plt.savefig(\"ROC_FP_OvO.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "\n",
    "\n",
    "print(roc_curves.keys())\n",
    "exp = roc_curves['small_regular']\n",
    "\n",
    "reg_counter = 0\n",
    "ma_counter = 0\n",
    "un_counter = 0\n",
    "\n",
    "for expert in EXPERT_IDS['human_ids']:\n",
    "    setlabel = None\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        color = pal[0]\n",
    "        if(reg_counter == 0):\n",
    "            setlabel = 'standard'\n",
    "            reg_counter +=1\n",
    "    elif expert.split('#')[0] == 'model_agreeing':\n",
    "        color = pal[1]\n",
    "        if(ma_counter == 0):\n",
    "            setlabel = 'agreeing'\n",
    "            ma_counter+=1\n",
    "    elif expert.split('#')[0] == 'unfair':\n",
    "        color = pal[2]\n",
    "        if(un_counter == 0):\n",
    "            setlabel = 'unfair'\n",
    "            un_counter += 1\n",
    "    plt.plot(exp[expert]['fn_fpr'],exp[expert]['fn_tpr'], c = color, linewidth = 0.4, label = setlabel)\n",
    "\n",
    "plt.plot(np.arange(0,1,0.01), np.arange(0,1,0.01), c = 'gray', linestyle = 'dashed')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title(f'ROC - False Negatives - One vs. Others')\n",
    "plt.legend()\n",
    "plt.savefig(\"ROC_FN_OvO.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibration of these estimates shouldnt be done on the data that was used to train them, right?\n",
    "#In that case I would suggest using the data where the experts were tweaked, since their behavior is the same there anyways. It was used\n",
    "#to tweak the beta and alpha intercepts, but their behavior is the same on the rest of the data\n",
    "#I will use month 6 as it was the validation for said training, apparent in the main\n",
    "import pandas as pd\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "test_pred = pd.read_parquet('/mnt/home/jean.alves/Clean_May/experts/expert_info_30_05_2023/deployment_predictions.parquet')\n",
    "val = pd.read_parquet('/mnt/home/jean.alves/Clean_May/data/BAF.parquet')\n",
    "model = RMAs['small_regular']\n",
    "\n",
    "expert = 'regular#0'\n",
    "\n",
    "test = pd.read_parquet('../testbed/test/test.parquet')\n",
    "test = test.drop(columns = 'month')\n",
    "\n",
    "test_expert_pred = test_pred.loc[test.index]\n",
    "test['model_score'] = test_expert_pred['model#0']\n",
    "\n",
    "\n",
    "predicted_fp = pd.DataFrame(index = test.index, columns = EXPERT_IDS['human_ids'])\n",
    "predicted_fn = pd.DataFrame(index = test.index, columns = EXPERT_IDS['human_ids'])\n",
    "label_fp = pd.DataFrame(index = test.index, columns = EXPERT_IDS['human_ids'])\n",
    "label_fn = pd.DataFrame(index = test.index, columns = EXPERT_IDS['human_ids'])\n",
    "\n",
    "for expert in EXPERT_IDS['human_ids']:\n",
    "    test['assignment'] = expert \n",
    "    test = cat_checker(test, data_cfg['data_cols']['categorical'] + ['assignment'], cat_dict)\n",
    "\n",
    "    outcomes = pd.DataFrame()\n",
    "    outcomes['labels'] = test['fraud_label']\n",
    "    outcomes['decisions'] = test_expert_pred[expert]\n",
    "    outcomes['outcomes'] = outcomes.apply(lambda x:    get_outcome(label=x['labels'], pred=x['decisions']),\n",
    "                    axis=1,\n",
    "            )\n",
    "    \n",
    "    pred_proba = model.expert_model.predict_proba(test.drop(columns = 'fraud_label'))\n",
    "\n",
    "    label_fp[expert]=(outcomes['outcomes'] == 'fp').astype(int)\n",
    "    predicted_fp[expert]=pred_proba[:, model.expert_model.classes_ == 'fp'].squeeze()\n",
    "    label_fn[expert]=(outcomes['outcomes'] == 'fn').astype(int)\n",
    "    predicted_fn[expert]=pred_proba[:, model.expert_model.classes_ == 'fn'].squeeze()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"ASSIGNMENT_COL\n",
    "val['assignment'] = expert\n",
    "            \n",
    "val = cat_checker(val, data_cfg['data_cols']['categorical'] + ['assignment'], cat_dict)\n",
    "\n",
    "pred_proba = model.expert_model.predict_proba(val)\n",
    "\n",
    "y_true=(outcomes['outcomes'] == 'fp').astype(int)\n",
    "y_pred=pred_proba[:, model.expert_model.classes_ == 'fp'].squeeze()\n",
    "gb_y, gb_x = calibration_curve(y_true, y_pred)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(gb_x, gb_y, marker='.')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fn.melt()\n",
    "label_fn.melt()\n",
    "from sklearn.calibration import calibration_curve\n",
    "gb_y, gb_x = calibration_curve(label_fn.melt()['value'], predicted_fn.melt()['value'], n_bins=10)\n",
    "\n",
    "pal = sns.color_palette(['C0','C2', 'C3'])\n",
    "pal = pal.as_hex()\n",
    "\n",
    "plt.plot(np.arange(0,1,0.01), np.arange(0,1,0.01), c = 'gray', linestyle = 'dashed')\n",
    "# plot model reliability\n",
    "plt.plot(gb_x, gb_y, marker='.', color = pal[0])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title(f'Calibration Plot for FN probability estimates')\n",
    "plt.savefig(\"Cal_FN.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fn.melt()\n",
    "label_fn.melt()\n",
    "\n",
    "gb_y, gb_x = calibration_curve(label_fp.melt()['value'], predicted_fp.melt()['value'], n_bins = 10)\n",
    "\n",
    "plt.plot(np.arange(0,1,0.01), np.arange(0,1,0.01), c = 'gray', linestyle = 'dashed')\n",
    "# plot model reliability\n",
    "plt.plot(gb_x, gb_y, marker='.', color = pal[0])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title(f'Calibration Plot for FP probability estimates')\n",
    "plt.savefig(\"Cal_FP.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
