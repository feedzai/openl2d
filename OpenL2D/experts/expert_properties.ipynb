{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "deployment = pd.read_parquet('../data/BAF_deployment_score.parquet')\n",
    "train = pd.read_parquet('../data/BAF.parquet').loc[:deployment.index[0]-1,:]\n",
    "\n",
    "current_expert_properties = pd.read_parquet(f'./expert_info/expert_properties.parquet')\n",
    "current_w = pd.read_parquet(f'./expert_info/full_w_table.parquet')\n",
    "current_deploy = pd.read_parquet(f'./expert_info/deployment_predictions.parquet')\n",
    "current_train = pd.read_parquet(f'./expert_info/train_predictions.parquet')\n",
    "current_perror = pd.read_parquet(f'./expert_info/p_of_error.parquet')\n",
    "current_deployment_exp_X = pd.read_parquet(f'./transformed_data/X_deployment_experts.parquet')\n",
    "current_train_exp_X = pd.read_parquet(f'./transformed_data/X_train_experts.parquet')\n",
    "\n",
    "with open(f'../ml_model/model/model_properties.pickle', 'rb') as infile:\n",
    "    ml_model_properties = pickle.load(infile)\n",
    "\n",
    "ml_model_threshold = ml_model_properties['threshold']\n",
    "ml_model_recall = 1 - ml_model_properties['fnr']\n",
    "ml_model_fpr_diff = ml_model_properties['disparity']\n",
    "\n",
    "pal = sns.color_palette(['C0','C2', 'C3'])\n",
    "pal = pal.as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_expert_properties['expert'].to_list()\n",
    "\n",
    "new_names = []\n",
    "for expert in current_expert_properties['expert'].to_list():\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        new_names.append('standard#' + expert.split('#')[1])\n",
    "    else:\n",
    "        new_names.append(expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = current_perror.loc[deployment.index, :]\n",
    "\n",
    "temp_lp = temp.loc[deployment.loc[deployment['fraud_bool'] == 1].index,:]\n",
    "temp_ln = temp.loc[deployment.loc[deployment['fraud_bool'] == 0].index, :]\n",
    "temp_lp = temp_lp.iloc[:, np.arange(0,100,2)]\n",
    "temp_ln = temp_ln.iloc[:, np.arange(1,101,2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ln_sample = temp_ln.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_exp_class(exp):\n",
    "    exp_type = exp.split('#')[0].split('_')[-1]\n",
    "    if exp_type == 'regular':\n",
    "        exp_type = 'standard'\n",
    "    if exp_type == 'agreeing':\n",
    "        exp_type = 'model_agreeing'\n",
    "    return exp_type\n",
    "\n",
    "melted_ln = temp_ln_sample.melt()\n",
    "melted_lp = temp_lp.melt()\n",
    "melted_ln['Group'] = melted_ln['variable'].apply(obtain_exp_class)\n",
    "melted_lp['Group'] = melted_lp['variable'].apply(obtain_exp_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_ln\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.histplot(data = melted_ln, x = 'value', hue = 'Group', stat = 'probability', bins = 20, log_scale = (False,10), palette=[\"C0\", \"C2\", \"C3\"])\n",
    "plt.ylabel('Proportion of Cases')\n",
    "\n",
    "plt.xlabel('Probability of FP - Label Negatives')\n",
    "plt.savefig(\"perror_dist_ln.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "g = sns.histplot(data = melted_lp, x = 'value', hue = 'Group', stat = 'probability', bins = 20, log_scale = (False,10), palette=[\"C0\", \"C2\", \"C3\"])\n",
    "plt.ylabel('Proportion of Cases')\n",
    "\n",
    "plt.xlabel('Probability of FN - Label Positives')\n",
    "plt.savefig(\"perror_dist_lp.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list = current_deploy.columns.drop('model#0').to_list()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fpr_disparity = pd.DataFrame(index = exp_list, columns = ['target_fpr', 'fpr_train', 'fpr_train_y', 'fpr_train_o', 'fpr_deployment', 'fpr_deployment_y', 'fpr_deployment_o', 'target_fnr', 'fnr_train', 'fnr_train_y', 'fnr_train_o', 'fnr_deployment', 'fnr_deployment_y', 'fnr_deployment_o', 'disparity_train', 'disparity_deployment'])\n",
    "\n",
    "old_index_dep = deployment.loc[deployment['customer_age'] > 50].index\n",
    "yng_index_dep = deployment.loc[deployment['customer_age'] < 50].index\n",
    "\n",
    "old_index_train = train.loc[train['customer_age'] > 50].index\n",
    "yng_index_train = train.loc[train['customer_age'] < 50].index\n",
    "for exp in exp_list:\n",
    "\n",
    "    preds = current_deploy[exp]\n",
    "    labels = deployment['fraud_bool']\n",
    "\n",
    "    preds_o = current_deploy[exp].loc[old_index_dep]\n",
    "    labels_o = deployment['fraud_bool'].loc[old_index_dep]\n",
    "\n",
    "    preds_y = current_deploy[exp].loc[yng_index_dep]\n",
    "    labels_y = deployment['fraud_bool'].loc[yng_index_dep]\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels,preds).ravel()\n",
    "\n",
    "    tn_o, fp_o, fn_o, tp_o = confusion_matrix(labels_o,preds_o).ravel()\n",
    "\n",
    "    tn_y, fp_y, fn_y, tp_y = confusion_matrix(labels_y,preds_y).ravel()\n",
    "\n",
    "    fpr_disparity.loc[exp, 'fpr_deployment'] = fp/(tn+fp)\n",
    "    fpr_disparity.loc[exp, 'fpr_deployment_y'] = fp_y/(tn_y+fp_y)\n",
    "    fpr_disparity.loc[exp, 'fpr_deployment_o'] = fp_o/(tn_o+fp_o)\n",
    "    fpr_disparity.loc[exp, 'fnr_deployment'] = fn/(fn+tp)\n",
    "    fpr_disparity.loc[exp, 'fnr_deployment_y'] = fn_y/(tp_y+fn_y)\n",
    "    fpr_disparity.loc[exp, 'fnr_deployment_o'] = fn_o/(tp_o+fn_o)\n",
    "\n",
    "    preds = current_train[exp]\n",
    "    labels = train['fraud_bool']\n",
    "\n",
    "    preds_o = current_train[exp].loc[old_index_train]\n",
    "    labels_o = train['fraud_bool'].loc[old_index_train]\n",
    "\n",
    "    preds_y = current_train[exp].loc[yng_index_train]\n",
    "    labels_y = train['fraud_bool'].loc[yng_index_train]\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels,preds).ravel()\n",
    "\n",
    "    tn_o, fp_o, fn_o, tp_o = confusion_matrix(labels_o,preds_o).ravel()\n",
    "\n",
    "    tn_y, fp_y, fn_y, tp_y = confusion_matrix(labels_y,preds_y).ravel()\n",
    "\n",
    "    fpr_disparity.loc[exp, 'fpr_train'] = fp/(tn+fp)\n",
    "    fpr_disparity.loc[exp, 'fpr_train_y'] = fp_y/(tn_y+fp_y)\n",
    "    fpr_disparity.loc[exp, 'fpr_train_o'] = fp_o/(tn_o+fp_o)\n",
    "    fpr_disparity.loc[exp, 'fnr_train'] = fn/(fn+tp)\n",
    "    fpr_disparity.loc[exp, 'fnr_train_y'] = fn_y/(tp_y+fn_y)\n",
    "    fpr_disparity.loc[exp, 'fnr_train_o'] = fn_o/(tp_o+fn_o)\n",
    "\n",
    "\n",
    "fpr_disparity['disparity_train'] = fpr_disparity['fpr_train_o'] - fpr_disparity['fpr_train_y']\n",
    "fpr_disparity['disparity_deployment'] = fpr_disparity['fpr_deployment_o'] - fpr_disparity['fpr_deployment_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_disparity['target_fpr'] = current_expert_properties['fpr_base'].values\n",
    "fpr_disparity['target_fnr'] = current_expert_properties['fnr_base'].values\n",
    "fpr_disparity.to_parquet(f'./expert_info/fpr_disparity.parquet')\n",
    "types = fpr_disparity.index.to_list()\n",
    "groups = []\n",
    "for expert in types:\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        groups.append('standard')\n",
    "    else:\n",
    "        groups.append(expert.split('#')[0])\n",
    "  \n",
    "fpr_disparity['group'] = groups\n",
    "\n",
    "fpr_disparity['tpr_deployment'] = 1 - fpr_disparity['fnr_deployment']\n",
    "fpr_disparity['target_tpr'] = 1 - fpr_disparity['target_fnr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = pd.read_parquet('/mnt/home/jean.alves/Clean_May_copy/experts/expert_info_30_05_2023/full_w_table.parquet')\n",
    "\n",
    "w = prop.drop(columns =['fp_intercept', 'fn_intercept', 'alpha'])\n",
    "w.index = pd.Index(new_names)\n",
    "w = w.div(np.sqrt(np.square(w).sum(axis=1)), axis = 0)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.heatmap(w.T, robust=True, cmap = 'coolwarm', vmax = 1, vmin = -1, cbar_kws = dict(use_gridspec=False,location=\"top\"))\n",
    "plt.title(\"Normalized Weight Vector Heatmap\")\n",
    "plt.savefig(\"w_heatmap.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.scatterplot(data=fpr_disparity, x='target_fpr', y='target_tpr', hue='group', style='group',  palette=[\"C0\", \"C2\", \"C3\"], s = 70).set(title = 'Recall vs FPR')\n",
    "\n",
    "plt.ylim([0.3,0.8])\n",
    "plt.xlim([0.01, 0.09])\n",
    "plt.savefig(\"Target_Performance.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.scatterplot(data=fpr_disparity, x='fpr_deployment', y='tpr_deployment', hue='group', style='group', s = 70, palette=[\"C0\", \"C2\", \"C3\"])\n",
    "fpr_model = 0.0451\n",
    "tpr_model = 0.5206\n",
    "sns.scatterplot(x= [fpr_model], y = [tpr_model], label = 'model', s = 70, color = 'black', marker = 'X').set(title = 'Recall vs FPR')\n",
    "plt.ylim([0.3,0.8])\n",
    "plt.xlim([0.01, 0.09])\n",
    "\n",
    "plt.savefig(\"Deployment_Performance.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_pred = current_deploy.drop(columns = 'model#0')\n",
    "model_pred = (current_deploy['model#0'] > ml_model_threshold).astype(int)\n",
    "labels = deployment['fraud_bool']\n",
    "exp_pred_lp = exp_pred.loc[labels == 1]\n",
    "exp_pred_ln = exp_pred.loc[labels == 0]\n",
    "model_pred_lp = model_pred.loc[labels == 1]\n",
    "model_pred_ln = model_pred.loc[labels == 0]\n",
    "\n",
    "exp_corr_mod_fp = []\n",
    "exp_corr_mod_fn = []\n",
    "exp_fp_mod_corr = []\n",
    "exp_fn_mod_corr = []\n",
    "\n",
    "expert_correct_model_fp = pd.DataFrame(index = exp_pred_ln.index, columns = exp_pred_ln.columns)\n",
    "\n",
    "expert_correct_model_fn = pd.DataFrame(index = exp_pred_lp.index, columns = exp_pred_lp.columns)\n",
    "\n",
    "expert_fp_model_correct = pd.DataFrame(index = exp_pred_ln.index, columns = exp_pred_ln.columns)\n",
    "\n",
    "expert_fn_model_correct = pd.DataFrame(index = exp_pred_lp.index, columns = exp_pred_lp.columns)\n",
    "\n",
    "for exp in exp_pred.columns:\n",
    "    expert_correct_model_fp[exp] = ((exp_pred_ln[exp] == 0) & (model_pred_ln == 1)).astype(int)\n",
    "    expert_correct_model_fn[exp] = ((exp_pred_lp[exp] == 1) & (model_pred_lp == 0)).astype(int)\n",
    "    expert_fp_model_correct[exp] = ((exp_pred_ln[exp] == 1) & (model_pred_ln == 0)).astype(int)\n",
    "    expert_fn_model_correct[exp] = ((exp_pred_lp[exp] == 0) & (model_pred_lp == 1)).astype(int)\n",
    "\n",
    "exp_corr_mod_fp = expert_correct_model_fp.mean()\n",
    "exp_corr_mod_fn = expert_correct_model_fn.mean()\n",
    "exp_fp_mod_corr = expert_fp_model_correct.mean()\n",
    "exp_fn_mod_corr = expert_fn_model_correct.mean()\n",
    "\n",
    "groups = []\n",
    "for expert in exp_corr_mod_fn.index.to_list():\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        groups.append('standard')\n",
    "    else:\n",
    "        groups.append(expert.split('#')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complementarity = pd.DataFrame(index = exp_corr_mod_fn.index)\n",
    "\n",
    "complementarity['L1E1M0'] = exp_corr_mod_fn\n",
    "complementarity['L1E0M1'] = exp_fn_mod_corr\n",
    "complementarity['L0E1M0'] = exp_fp_mod_corr\n",
    "complementarity['L0E0M1'] = exp_corr_mod_fp\n",
    "complementarity['group'] = groups\n",
    "complementarity.groupby('group').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_over_all_cases = pd.DataFrame(index = exp_list + ['model'], columns = ['cost'])\n",
    "lam = ml_model_threshold/(1-ml_model_threshold)\n",
    "for expert in exp_list:\n",
    "    fn = (1 - exp_pred_lp[expert])\n",
    "    fp = exp_pred_ln[expert]\n",
    "\n",
    "    costs_over_all_cases.loc[expert, 'cost'] = fn.sum() + fp.sum()*lam\n",
    "\n",
    "mod_fn = 1- model_pred_lp\n",
    "mod_fp = model_pred_ln\n",
    "costs_over_all_cases.loc['model', 'cost'] = mod_fn.sum() + mod_fp.sum()*lam\n",
    "\n",
    "costs_over_all_cases.reset_index(inplace=True)\n",
    "\n",
    "for expert in costs_over_all_cases['index']:\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        costs_over_all_cases.replace(to_replace=expert, value= 'standard#' + expert.split('#')[1], inplace=True)\n",
    "\n",
    "\n",
    "costs_over_all_cases.sort_values(by = 'cost', inplace = True)\n",
    "\n",
    "groups = []\n",
    "for expert in costs_over_all_cases['index'].to_list():\n",
    "    if expert == 'model':\n",
    "        groups.append('C7')\n",
    "    else:\n",
    "        if expert.split('#')[0] == 'standard':\n",
    "            groups.append('C0')\n",
    "        if expert.split('#')[0] == 'model_agreeing':\n",
    "            groups.append('C2')\n",
    "        if expert.split('#')[0] == 'unfair':\n",
    "            groups.append('C3')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "sns.barplot(data = costs_over_all_cases, orient = 'h', y = 'index', x = 'cost', palette=groups, width = 0.7)\n",
    "plt.xlabel(\"Loss over Deployment Split\")\n",
    "plt.ylabel(\"Expert\")\n",
    "plt.title('Expert vs Model Performance')\n",
    "plt.savefig(\"Loss_over_deployment.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "CK_model = pd.DataFrame(index = exp_pred.columns.to_list(), columns = ['expert', 'ck'])\n",
    "CK_model['expert'] = exp_pred.columns.to_list()\n",
    "for expert in exp_pred.columns:\n",
    "    CK_model.loc[expert, 'ck'] = cohen_kappa_score(exp_pred[expert], model_pred)\n",
    "\n",
    "\n",
    "CK_model.loc['model', 'ck'] = 1\n",
    "CK_model.loc['model', 'expert'] = 'model'\n",
    "temp = fpr_disparity.reset_index().loc[:,['index', 'disparity_deployment']]\n",
    "temp.loc[50] = ['model', ml_model_fpr_diff]\n",
    "\n",
    "for expert in CK_model['expert']:\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        CK_model.replace(to_replace=expert, value= 'standard#' + expert.split('#')[1], inplace=True)\n",
    "for expert in temp['index']:\n",
    "    if expert.split('#')[0] == 'regular':\n",
    "        temp.replace(to_replace=expert, value= 'standard#' + expert.split('#')[1], inplace=True)\n",
    "\n",
    "a = temp['index'].to_list()\n",
    "groups = []\n",
    "for expert in a:\n",
    "    if expert == 'model':\n",
    "        groups.append('C7')\n",
    "    else:\n",
    "        if expert.split('#')[0] == 'standard':\n",
    "            groups.append('C0')\n",
    "        if expert.split('#')[0] == 'model_agreeing':\n",
    "            groups.append('C2')\n",
    "        if expert.split('#')[0] == 'unfair':\n",
    "            groups.append('C3')\n",
    "\n",
    "f, ax = plt.subplots(1,2, figsize=(9, 9), sharey = True)\n",
    "\n",
    "sns.barplot(ax = ax[0], data = CK_model, orient = 'h', y = 'expert', x = 'ck', palette = groups)\n",
    "for bar in ax[0].patches:\n",
    "    if bar.get_width() == 1:\n",
    "        bar.set_color('black')    \n",
    "\n",
    "ax[0].set_title(\"Cohen's Kappa: Expert vs. Model\")\n",
    "ax[0].set_xlabel(\"Cohen's Kappa\")\n",
    "ax[0].set_ylabel('Expert')\n",
    "\n",
    "sns.barplot(ax = ax[1], data = temp, orient = 'h', y = 'index', x = 'disparity_deployment', palette = groups).set(ylabel = None)\n",
    "for bar in ax[1].patches:\n",
    "    if bar.get_width() == ml_model_fpr_diff:\n",
    "        bar.set_color('black')    \n",
    "\n",
    "ax[1].set_title(\"FPR Disparity: Old vs. Young\")\n",
    "ax[1].set_xlabel(\"FPR Disparity\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0)\n",
    "    \n",
    "plt.savefig(\"Cohen_Kappa.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
